---
title: "create_seasonality_archetypes_Nigeria_DS"
author: "Monique Ambrose"
date: "October 18, 2019"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
    number_sections: true
---

```{r commented_intro, include=FALSE, echo=FALSE}

#################################################################################################
# High burden, high impact -> Nigeria, 2019 -> Spatial Clustering
#
# Master script that coordinates the creation of level 1 archetypes for Nigeria. Clusters
#    should contain DS which are expected to have similar values for the variables included 
#    in clustering, such as seasonality and vector composition.
#    
#
# Steps:
# 1) Read in relevant rastor files that will serve as the clustering variables. Clip and mask 
#     to relevant geographic area. Align so that each cell in one raster matches with 
#     the same location in all other rasters.
# 2) Rescale the range of values for each variable (to avoid weighting one variable as more 
#     important simply because there is a larger range of values). Weight each variable 
#     according to expected/desired importance by adjusting the range.
# 3) Singular value decomposition (SVD) - not currently used, but may be added if the number 
#     of variables increases and clustering takes too long.
# 4) Find mean pixel value among all pixels in a DS, for each variable layer
# 4) Clustering step: any one of a number of clustering algorithms could be used here to find 
#    the DS that are most similar to one another (based on mean DS variable values). Currently,
#    k-means and CLARA algorithms are used.
#
# Note: this is a relatively simplistic and targeted treatment of the problem. For a more thorough, 
#    flexible, and general approach, I recommend looking at scripts in Amelia Bertozzi-Villa's 
#    malaria-atlas-project/seasonal_classification repo:
#  https://github.com/InstituteforDiseaseModeling/malaria-atlas-project/tree/master/seasonal_classification
#
#################################################################################################


```

# Introduction 

## Project goals and general approach

The broader goal of this project is to explore the effect of different intervention packages in each of the Nigerian health districts (DS). Here, we use a clustering approach to partition the health districts into clusters or archetypes (each DS will belong to one of the archetypes) with similar characteristics (e.g., seasonality and vector composition).



</br>


## This document

This document walks through the steps taken to perform the clustering, form curating the input files, to generating clusters, to exploring the values of variables in each cluster.

  1) Prepare raster layers (input variables) and health district shapefile
  
    * Create shapefile for health districts with the same CRS as input rasters
    * Create land use rasters
    * Standardize projection and resolution (align pixels) for all raster layers
    * Rescale each layer according to the importance/weight we want it to have when clusters are formed
    
    * Find mean variable values among all pixels in a DS


  2) Run clustering algorithm
  
  3) Explore clustering results


</br>



# Setup and file locations

<font size="5">   - Code </font>

  * Git repo [here](https://github.com/numalariamodeling/hbhi-spatial-clustering)
  

<font size="5">   - Input raster files </font>


  * Land use raster - shared by Navideh through Dropbox folder: LandUse300m/ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7.tif [here](https://www.dropbox.com/home/LandUse300m)
  
  
  * All other input rasters were saved directly in subdirectories of hbhi_nigeria/SpatialClustering/input_layers
  
    * Rainfall and tsi rasters from Amelia's folder in Dropbox: Malaria Team Folder/projects/map_intervention_impact/seasonal_classification/africa/rasters
    * ITN rasters from Amelia's folder in Dropbox: Malaria Team Folder/projects/map_itn_cube/20190808_new_landcover
    * Vector and PfPR(2-10) rasters downloaded from [MAP's website](https://map.ox.ac.uk/explorer/#/) July 2019.




<font size="5">   - Input shape files </font>

  * Country and DS borders: nigeria_shapefiles/Nigeria LGAs shapefile 191016/NGA_LGAs.shp


<font size="5">   - Directory setup for raster file inputs and clustering results </font>

Within a directory named SpatialClustering, subdirectories must be as follows:

  * aligned_layers
  * clustering_results
  * input_layers - must have one subdirectory for each variable type, each of which must contain relevant rasters
    * climate
    * land
    * vector
    * pfpr
    * itn
  * unscaled_layers
  




</br>



</br>



# Prepare boundary shapefiles and raster layers

## Health district shapefile

Transform existing shapefile with health districts so that it has the same CRS as the raster files that will be used as clustering variables

```{r DS_boundaries_shapefile, results='hide', message=FALSE, warning=FALSE}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# -                               Setup                                 - #
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #

library(rgdal)
library(raster)
library(stats)
library(viridis)
library(factoextra)
library(plotrix)


# set base filepaths
base_filepath_scripts = 'C:/Users/mambrose/Documents/hbhi-spatial-clustering'
land_use_filepath = 'C:/Users/mambrose/Dropbox (IDM)/LandUse300m'
dhs_cluster_locations_base_filepath = 'C:/Users/mambrose/Box/nigeria_dhs/data_analysis/data'  # previous location: 'C:/Users/mambrose/Box/hbhi_nigeria/data'
dhs_cluster_numkids_base_filepath = 'C:/Users/mambrose/Box/hbhi_nigeria/cluster DHS estimates/U5/fever/Med_fever'  # previous location: 'C:/Users/mambrose/Box/hbhi_nigeria/cluster DHS estimates/fever/Med_fever'

dhs_cluster_years = c(2003, 2008, 2010, 2013, 2015)
dhs_index_reassignment = 3  # which survey should be used when reassigning archetypes with too few surveyed U5
dhs_cluster_years_2 = c('03', '08', '10', '13', '15')
survey_type_string = c('DHS', 'DHS', 'MIS', 'DHS', 'MIS')
dhs_shapefile_names = c('NGGE4BFL/NGGE4BFL.shp', 'NGGE52FL/NGGE52FL.shp', 'NGGE61FL/NGGE61FL.shp', 'NGGE6AFL/NGGE6AFL.shp', 'NGGE71FL/NGGE71FL.shp')
dhs_cluster_locations_filepath = paste(dhs_cluster_locations_base_filepath, '/NG_',dhs_cluster_years, '_', survey_type_string,'_06192019/', dhs_shapefile_names, sep='')
dhs_cluster_numkids_filepath = paste(dhs_cluster_numkids_base_filepath, '/clu_fever_',dhs_cluster_years_2, '.csv', sep='')

# which admin level should be used?
nigeria_admin = 0
if(nigeria_admin == 1){
  base_filepath_raster = 'C:/Users/mambrose/Box/hbhi_nigeria/SpatialClustering_admin1'

  ds_shapefile_unprojected_filepath = 'C:/Users/mambrose/Box/nigeria_shapefiles/nigeria_polbnda_admin_1_unsalb/Admin_1/NGA_cnty_admin1/nga_polbnda_adm1_1m_salb.shp'  
  ds_shapefile_filepath = 'C:/Users/mambrose/Box/hbhi_nigeria/SpatialClustering_admin1/reference_rasters_shapefiles/nga_polbnda_adm1_1m_salb.shp'
  
  # specify the name of shapefile's attribute that identifies the name of the districts/admin2s (or whatever admin level is being used for clustering)
  admin_attribute_name = 'ADM1_NAME'  # 'NOMDEP'
  region_attribute_name = 'ADM1_CODE' # 'NOMREGION'

} else{
  base_filepath_raster = 'C:/Users/mambrose/Box/hbhi_nigeria/SpatialClustering'

  # ds_shapefile_unprojected_filepath = 'C:/Users/mambrose/Box/nigeria_shapefiles/Nigeria LGAs shapefile (260216)/NGA_LGAs.shp'
  ds_shapefile_unprojected_filepath = 'C:/Users/mambrose/Box/nigeria_shapefiles/Nigeria LGAs shapefile 191016/NGA_LGAs.shp'
  ds_shapefile_filepath = 'C:/Users/mambrose/Box/hbhi_nigeria/SpatialClustering/reference_rasters_shapefiles/NGA_DS_clusteringProjection.shp'
  
  # specify the name of shapefile's attribute that identifies the name of the districts/admin2s (or whatever admin level is being used for clustering)
  admin_attribute_name = 'LGA'  # 'NOMDEP'
  region_attribute_name = 'ADM1_NAME' # 'NOMREGION'
}


montly_case_data_filepath = 'C:/Users/mambrose/OneDrive - IDMOD/HBHI_BurkinaFaso/monthly_lga_14-18.csv'
unmasked_reference_raster_filename = paste(base_filepath_raster, '/input_layers/climate/rainfall_month_01.tif', sep='')
reference_raster_filename = paste(base_filepath_raster, '/reference_rasters_shapefiles/cropped_projection_reference.tif', sep='')


setwd(base_filepath_scripts)


# specify information for the land-use raster creation
num_land_types = 6
overwrite_file_flag = FALSE
# determine whether aligned/scaled rasters should be overwritten if they already exist
overwrite_raster_flag = FALSE
# specify the maximum number of clusters to create (e.g. for elbow plot)
max_num_clusters = 60

num_clusters_plot = c(10, 20, 40, 60)




# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# -   Prepare reference raster and shapefile with appropriate projection and masking    - #
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# check whether the file already exists
if ((!file.exists(ds_shapefile_filepath)) | (!file.exists(reference_raster_filename))){
  # read in DS shapefile with different projection than raster files
  ds_shapefile_unprojected = shapefile(ds_shapefile_unprojected_filepath)
  # change the shapefile's admin attribute name to 'NOMDEP' so that it is referenced correctly in this script
  names(ds_shapefile_unprojected)[names(ds_shapefile_unprojected) == admin_attribute_name] = 'NOMDEP'
  names(ds_shapefile_unprojected)[names(ds_shapefile_unprojected) == region_attribute_name] = 'NOMREGION'
  # read in one of the input raster files (projection and resolution will be used for all future rasters/shapefiles)
  unmasked_raster = raster(unmasked_reference_raster_filename)
  # need to reproject shapefile so that it will have the same projection as the reference raster
  ds_shapefile = spTransform(ds_shapefile_unprojected, crs(unmasked_raster))
  # save shapefile as country/ds shapefile for all raster creations
  shapefile(ds_shapefile, filename=ds_shapefile_filepath)
  # crop and mask the raster to the current country borders
  cropped_projection_reference = crop(unmasked_raster, ds_shapefile)
  cropped_projection_reference = mask(cropped_projection_reference, ds_shapefile)
  # save reference raster
  writeRaster(cropped_projection_reference, reference_raster_filename)
}
```

Saves a reference raster and shapefile (to be used for the projection and resolution in future analyses) in reference_rasters_shapefiles folder.

</br>



</br>


## Create land use rasters

```{r land_use, results='hide', message=FALSE, warning=FALSE}
source('prepare_clustering_inputs/createLandUseRasters.R')

create_land_use_rasters(ds_shapefile_filepath=ds_shapefile_filepath, land_use_filepath=land_use_filepath, num_land_types=num_land_types, overwrite_file_flag=overwrite_file_flag)

```

Generates several rasters, which are saved in input_layers/land: one for each of the most common land types in the original land-use raster. The original raster used categorical variables, so we separate into component rasters for clustering: a pixel takes value 1 if it has a given land type and 0 otherwise. 


</br>




</br>


## Calculate the mean monthly cases in each DS

```{r monthly_cases_each_ds, results='hide', message=FALSE, warning=FALSE, fig.width=8, fig.height=6, fig.show='hold'}
source('prepare_clustering_inputs/calculate_mean_monthly_cases_each_DS.R')

calculate_mean_monthly_cases_each_DS(base_filepath_raster, ds_shapefile_filepath, montly_case_data_filepath, var_weight_string, plot_flag=TRUE, use_susp_flag=TRUE)

```

<b> Each line shows the mean monthly suspected+confirmed cases per 1000 (left) or rescaled according to the maximum cases in a year (right) for a DS, colored according to DS's region.</b> 

The generated csv and raster files are saved in input_layers/caseSeasonalityDS.



</br>


## Align and weight/rescale all raster layers

Weights for each variable type:

```{r specify_variable_type_weights, echo=FALSE}


# specify variable types and desired weights
# options are
#   'climate' - create archetypes based on climate alone
#   'climate_vector' - create archetypes based on climate and vector
#   'climate_vector_lane' - create archetypes based on climate, vector, and land
#   'case_seasonality' - only use the seasonality patterns of (rescaled) cases in each DS to create archetypes
#   'climate_vector_case_seasonality' - create archetypes based on climate, vector, and case seasonality
clustering_type = 'vector_rain_tsi_pre2010MAP3pfpr_pre2010itn'
# clustering_type = 'vector_rain_tsi_pre2010pfpr_pre2010itn'  

#------------------------------------#
# seasonality-archetype clusterings
#------------------------------------#
if (clustering_type == 'climate'){
  variable_type_names = c('climate')
  variable_type_weights = c(5)
  
} else if(clustering_type == 'climate_vector'){
  variable_type_names = c('vector', 'climate')
  variable_type_weights = c(2, 4)
  
} else if(clustering_type == 'climate_vector_land'){
  variable_type_names = c('vector', 'climate', 'land')
  variable_type_weights = c(2, 4, 1)
  
}else if(clustering_type == 'case_seasonality'){
  variable_type_names = c('rescaledCaseSeasonalityDS')
  variable_type_weights = c(1)
  
} else if (clustering_type == 'climate_vector_case_seasonality'){
  variable_type_names = c('vector', 'climate', 'rescaledCaseSeasonalityDS')
  variable_type_weights = c(2, 4, 4)
  
} else if (clustering_type == 'climate_vector_pfpr'){
  variable_type_names = c('vector', 'climate', 'pfpr')
  variable_type_weights = c(2, 4, 4)
  
} else if (clustering_type == 'climate_vector_pfpr_itn'){
  variable_type_names = c('vector', 'climate', 'pfpr', 'itn')
  variable_type_weights = c(2, 4, 3, 1)  # c(2, 4, 4, 3)
  
} else if (clustering_type == 'climate_vector_pre2010pfpr_pre2010itn'){
  variable_type_names = c('vector', 'climate', 'pre2010pfpr', 'pre2010itn')
  variable_type_weights = c(2, 4, 3, 1)  # c(2, 4, 4, 3)
  
} else if (clustering_type == 'pfpr_itn'){
  variable_type_names = c('pfpr', 'itn')
  variable_type_weights = c(3, 1)
  
} else if (clustering_type == 'pre2010MAP3pfpr'){
  variable_type_names = c('pre2010MAP3pfpr')
  variable_type_weights = c(3)
  
} else if (clustering_type == 'pre2010pfpr_pre2010itn'){
  variable_type_names = c('pre2010pfpr', 'pre2010itn')
  variable_type_weights = c(3, 1)  # c(2, 4, 4, 3)
  
} else if (clustering_type == 'vector_rain_tsi_pre2010pfpr_pre2010itn'){
  variable_type_names = c('vector', 'rain', 'tsi', 'pre2010pfpr', 'pre2010itn')
  variable_type_weights = c(2, 3, 2, 3, 1)  # c(2, 4, 4, 3)
  
} else if (clustering_type == 'vector_rain_tsi_pre2010MAP3pfpr_pre2010itn'){
  variable_type_names = c('vector', 'rain', 'tsi', 'pre2010MAP3pfpr', 'pre2010itn')
  variable_type_weights = c(2, 3, 2, 3, 1)  # c(2, 4, 4, 3)
  
} else{
  warning('clustering type not recognized')
} 

# weights for each variable type included in filepath
var_weight_string = paste(variable_type_names[1], variable_type_weights[1], sep='')
if(length(variable_type_names)>1){
  for (i_var in 2:length(variable_type_names)){
    var_weight_string = paste(var_weight_string, '_', variable_type_names[i_var], variable_type_weights[i_var], sep='')
  }
}


# check if the directories for this combination of variable weights exist; if not, create the directories
if (!dir.exists(paste(base_filepath_raster, '/aligned_layers/', var_weight_string, sep=''))){
  dir.create(paste(base_filepath_raster, '/aligned_layers/', var_weight_string, sep=''))
  print('creating new aligned_layers/ directory for this variable weighting')
}
if (!dir.exists(paste(base_filepath_raster, '/clustering_results/', var_weight_string, sep=''))){
  dir.create(paste(base_filepath_raster, '/clustering_results/', var_weight_string, sep=''))
  dir.create(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/figures', sep=''))
  print('creating new clustering_results/ directory for this variable weighting')
}


# output message to view variable weights
for (i_var in 1:length(variable_type_names)){
  print(paste(variable_type_names[i_var], ': ', variable_type_weights[i_var], sep=''))
}
```


Read in, clip, mask, align, and weight the rastor files that will serve as the clustering variables:

```{r prepare_raster_inputs, results='hide', message=FALSE, warning=FALSE}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# -               Prepare raster input files                            - #
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #

# load raster-preparation functions
source('prepare_clustering_inputs/extract_raster_values.R')

# list of all .tiff files in directories
variable_type_files = list()
for(vv in 1:length(variable_type_names)){
  variable_type_files[[vv]] =  list.files(path=paste(base_filepath_raster, '/input_layers/', variable_type_names[vv], sep=''), pattern="*.{0,3}tiff?", full.names=TRUE, recursive=FALSE)
  # remove any files that contain '.aux.xml'
  remove_indices = grep('.aux.xml', variable_type_files[[vv]])
  if (length(remove_indices)>0){
    variable_type_files[[vv]] = variable_type_files[[vv]][-remove_indices]
  }
}

# read in, clip, mask, align, and weight the rastor files that will serve as the clustering variables
align_input_rasters(base_filepath_raster=base_filepath_raster, 
                    ds_bf_shapefile_filepath=ds_shapefile_filepath, 
                    variable_type_names=variable_type_names,
                    variable_type_weights=variable_type_weights, 
                    variable_type_files=variable_type_files, 
                    var_weight_string = var_weight_string,
                    overwrite_raster_flag=overwrite_raster_flag)


# read in dataframe that gives information on the variables, variable types, and weights (csv file created in align_input_rasters() function)
variable_record_df = read.csv(paste(base_filepath_raster, '/aligned_layers/', var_weight_string, '/variable_information.csv', sep=''))

# store vector of unscaled and rescaled ('aligned') raster filenamess - keep in vector in same order as saved in variable_record_df csv file created in align_input_rasters() function
unscaled_files = c()
aligned_files = c()
for(ii in 1:length(variable_record_df[,1])){
  unscaled_files = c(unscaled_files, paste(base_filepath_raster, '/unscaled_layers/', sub('aligned', 'unscaled', variable_record_df$variable_filename_vector[ii]), sep=''))
  aligned_files = c(aligned_files, paste(base_filepath_raster, '/aligned_layers/', var_weight_string, '/', variable_record_df$variable_filename_vector[ii], sep=''))
}

```


Find the mean variable values among all pixels in the same DS and save results as a csv file:

```{r get_DS_means, message=FALSE, warning=FALSE}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# -                  Mean values for each DS                            - #
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# cluster by DS - for each DS, use mean value across all pixels (unweighted by population sizes)
source('prepare_clustering_inputs/cluster_by_DS_mean.R')

get_DS_means(base_filepath_raster, ds_shapefile_filepath, variable_record_df, var_weight_string, aligned_files)

```



# Run clustering algorithm

Use k-means and the CLARA algorithms to perform clustering - repeat with different numbers of total clusters:

```{r run_clustering_algorithms, message=FALSE, warning=FALSE}


# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# -                  Clustering at DS level                             - #
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
num_clusters_vect = 1:max_num_clusters
create_new_clusters_flag = !file.exists(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_level_clusterResultObject_', max_num_clusters, 'clusters.RData', sep=''))
if (create_new_clusters_flag){
  kmeans_ss_DS = cluster_DS_means(base_filepath_raster, ds_shapefile_filepath, variable_record_df, var_weight_string, aligned_files, num_clusters_vect)
}




```








```{r, plot_cluster_output, echo = FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=12}
par(mfrow=c(3,2), mar=c(0.1, 0.1, 1, 0.1))

# load shapefile for health districts
ds_shapefile = shapefile(ds_shapefile_filepath)
ds_names = ds_shapefile$NOMDEP

for(ii in 1:length(num_clusters_plot)){
  num_clusters=num_clusters_plot[ii]

  
  
  # load in kmeans output for this number of clusters - kmeans_output and clara_output variables are loaded
  load(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_level_clusterResultObject_', num_clusters, 'clusters.RData', sep=''))
  # get the names of the DS used as medoids
  DS_representative_ids = clara_output$i.med
  
  # plot coloring DS according to archetype DS
  clust_cols = rainbow(num_clusters, alpha=0.2)[clara_output$clustering]
  clust_cols[DS_representative_ids] = rainbow(num_clusters, alpha=0.6)

  
  plot(ds_shapefile, col=clust_cols, main=paste(num_clusters,'archetypes'))

}


par(mfrow=c(1,1))

```

<b>Clustering results.</b> CLARA with different numbers of clusters. Each color corresponds to an archetype; the representative DS for each archetype is shown in a darker color. 



Representative DS when different numbers of clusters are used:

```{r print_representative_DS}
num_clusters_print = c(5)
for(ii in 1:length(num_clusters_print)){
  # load in clustering output for this number of clusters - kmeans_output and clara_output variables are loaded
  load(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_level_clusterResultObject_', num_clusters_print[ii], 'clusters.RData', sep=''))
  
  print(paste('representative DS for ', num_clusters_print[ii], ' archetypes: ', sep=''))
  print(ds_shapefile$NOMDEP[clara_output$i.med])

}


```

</br>

## Restore original variable values and assign to each cluster

```{r restore_variable_values, message=FALSE, warning=FALSE}


# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# -                   Record cluster centroids                          - #
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# create csv file indicating the centroid value of each variable in each cluster (to be used as inputs/parameters for DTK simulations)
#    the cluster output file provides the rescaled values of each variable, so need to restore them to their original values
for (num_clusters in num_clusters_plot){

  # load in kmeans output for this number of clusters - kmeans_output and clara_output variables are loaded
  load(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_level_clusterResultObject_', num_clusters, 'clusters.RData', sep=''))
  
  if (create_new_clusters_flag | !file.exists(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_kmeans_centroids_', num_clusters,'clusters.csv', sep='')) | !file.exists(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_clara_medoids_', num_clusters,'clusters.csv', sep=''))){
    # load functions for calculating centroid values for each cluster
    source('explore_clustering_results/restore_raster_values.R')
  
    # run function to calculate original/restored variable values for each of the cluster centroids and save as csv
    restore_raster_values(base_filepath_raster, var_weight_string, unscaled_files, variable_record_df, kmeans_output, num_clusters, clara_output, include_clara_flag=TRUE, pixel_flag = FALSE)
  }
  
  # read in results and print a part of the table
  clara_medoid_restored_values = read.csv(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_clara_medoids_', num_clusters,'clusters.csv', sep=''))
  colnames_centroid_restored_values = sub('_aligned.tif', '', as.character(variable_record_df$variable_filename_vector))
  
  # determine which columns to include in printed table
    
  if(clustering_type == 'climate_vector'){
    col_indices = c(which(variable_record_df$variable_type_vector=='vector'), which(variable_record_df$variable_type_vector=='climate')[c(4, 10)]) 
    
  } else if(clustering_type == 'climate_vector_land'){
    col_indices = c(which(variable_record_df$variable_type_vector=='vector'), which(variable_record_df$variable_type_vector=='climate')[c(4, 10)], which(variable_record_df$variable_type_vector=='land')[1]) 
    
  } else if(clustering_type == 'case_seasonality'){
    col_indices = c(which(variable_record_df$variable_type_vector=='rescaledCaseSeasonalityDS')[c(2, 6, 10)]) 
    
  } else if (clustering_type == 'climate_vector_case_seasonality'){
    col_indices = c(which(variable_record_df$variable_type_vector=='vector'), which(variable_record_df$variable_type_vector=='climate')[c(4, 10)], which(variable_record_df$variable_type_vector=='rescaledCaseSeasonalityDS')[c(2, 6, 10)])  # , which(variable_record_df$variable_type_vector=='land')[1]
  
  } else if(clustering_type == 'climate'){
      col_indices = c(which(variable_record_df$variable_type_vector=='climate')[c(1, 4, 7, 10)]) 
      
  } else if (clustering_type == 'climate_vector_pfpr'){
    col_indices = c(which(variable_record_df$variable_type_vector=='vector'), which(variable_record_df$variable_type_vector=='climate')[c(4, 10)], which(variable_record_df$variable_type_vector=='pfpr')[c(1,3)]) 
    
  } else if (clustering_type == 'climate_vector_pfpr_itn'){
    col_indices = c(which(variable_record_df$variable_type_vector=='vector'), which(variable_record_df$variable_type_vector=='climate')[c(4, 10)], which(variable_record_df$variable_type_vector=='pfpr')[c(1,3)], which(variable_record_df$variable_type_vector=='itn')[c(1,3)]) 
    
   }else if (clustering_type == 'climate_vector_pre2010pfpr_pre2010itn'){
    col_indices = c(which(variable_record_df$variable_type_vector=='vector'), which(variable_record_df$variable_type_vector=='climate')[c(4, 10)], which(variable_record_df$variable_type_vector=='pre2010pfpr')[c(1,3)], which(variable_record_df$variable_type_vector=='pre2010itn')[c(1,3)]) 
    
  } else if (clustering_type == 'pfpr_itn'){
    col_indices = c(which(variable_record_df$variable_type_vector=='pfpr')[c(1,3)], which(variable_record_df$variable_type_vector=='itn')[c(1,3)]) 
    
  } else if (clustering_type == 'vector_rain_tsi_pre2010pfpr_pre2010itn'){
    col_indices = c(which(variable_record_df$variable_type_vector=='vector'), which(variable_record_df$variable_type_vector=='rain')[c(4, 10)], which(variable_record_df$variable_type_vector=='pre2010pfpr')[c(1,3)], which(variable_record_df$variable_type_vector=='pre2010itn')[c(1,3)]) 
 
  } else if (clustering_type == 'vector_rain_tsi_pre2010MAP3pfpr_pre2010itn'){
    col_indices = c(which(variable_record_df$variable_type_vector=='vector'), which(variable_record_df$variable_type_vector=='rain')[c(4, 10)], which(variable_record_df$variable_type_vector=='pre2010MAP3pfpr')[c(1,3)], which(variable_record_df$variable_type_vector=='pre2010itn')[c(1,3)]) 
  }
}  
    
    
  # if the column name starts with a number, add an X to the front
  print_table_colnames = colnames_centroid_restored_values[col_indices]
  # first character
  first_chars = substr(print_table_colnames, 1, 1)
  # if the first character is a number, the new string will begin with X
  new_colnames = ifelse(grepl('[0-9]', first_chars), paste('X', print_table_colnames, sep=''), print_table_colnames)
  # also replace '-' with '.'
  new_colnames = gsub('-', '.', new_colnames)
  
  print(clara_medoid_restored_values[1:5, new_colnames])

  
```
Creates a csv file clustering_results/{parameter_weighting_of_interest}/DS_kmeans_centroids_{num_clusters}clusters.csv that gives the restored variable value for the centroid of each cluster. 


</br>

</br>


# Explore clustering results

## Elbow method

Explore the optimal number of clusters

```{r elbow_method, message=FALSE, warning=FALSE}


# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# -           Elbow method for determining number of clusters           - #
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# Calculate how much variance is explained for each number of clusters
pdf_flag=TRUE


if (create_new_clusters_flag){
  # save results in csv file
  kmeans_sum_squares_results = matrix(nrow=3, ncol=length(num_clusters_vect))
  kmeans_sum_squares_results[1,] = kmeans_ss_DS[[1]]  # betweenss_vect
  kmeans_sum_squares_results[2,] = kmeans_ss_DS[[2]]  # sum_withinss_vect
  kmeans_sum_squares_results[3,] = kmeans_ss_DS[[3]]  # totss_vect
  rownames(kmeans_sum_squares_results) = c('between_ss', 'sum_within_ss', 'total_ss')
  colnames(kmeans_sum_squares_results) = num_clusters_vect
  
  write.csv(kmeans_sum_squares_results, paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_kmeans_elbow_min_', min(num_clusters_vect), '_max', max(num_clusters_vect),'.csv', sep=''))
}

# determine whether to plot kmeans or clara result
plot_clara = TRUE

if(plot_clara){
  # read in dataframe of the weighted variable values for each DS (used in clustering)
  ds_means_df = read.csv(paste(base_filepath_raster, '/aligned_layers/', var_weight_string, '/mean_variable_values_each_DS.csv', sep=''))[,-1]
  
   if(pdf_flag){
    pdf_filename = paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/figures/optimal_number_clusters_clara.pdf', sep='')
    pdf(pdf_filename, width=6, height=4)
  }

  # create three plots: one for each of the methods to find the optimal number of clusters
  fviz_nbclust(as.matrix(ds_means_df[,-1]), clara, method = "wss", k.max=max(num_clusters_vect)) +
    labs(subtitle = "Elbow method")
  fviz_nbclust(as.matrix(ds_means_df[,-1]), clara, method = "silhouette", k.max=max(num_clusters_vect)) +
    labs(subtitle = "Silhouette method")
  fviz_nbclust(as.matrix(ds_means_df[,-1]), clara, method = "gap_stat", k.max=max(num_clusters_vect)) +
   labs(subtitle = "Gap statistic method")
  
  if(pdf_flag){
    dev.off()
  }
  
} else{
  # read in previously-saved results
  kmeans_sum_squares_results = read.csv(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_kmeans_elbow_min_', min(num_clusters_vect), '_max', max(num_clusters_vect),'.csv', sep=''), header=TRUE)[,-1]
  rownames(kmeans_sum_squares_results) = c('between_ss', 'sum_within_ss', 'total_ss')
  colnames(kmeans_sum_squares_results) = num_clusters_vect
  
  # create elbow plot to explore optimal number of clusters
  if(pdf_flag){
    pdf_filename = paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/figures/elbow_plot_kmeans.pdf', sep='')
    pdf(pdf_filename, width=10, height=6)
  }
  plot(num_clusters_vect, (kmeans_sum_squares_results[1,] / kmeans_sum_squares_results[3,]), type='b', bty='L', ylab='between SS / total SS', xlab='number of clusters')
  if(pdf_flag){
    dev.off()
  }
}


```

<b>Variance explained as a function of the number of clusters.</b> 


</br>

## Compare original variable values with clustered values

For each pixel in Burkina Faso, we can visually examine how similar the original, DS-mean, and cluster-assigned variable values are. The below plots show a select handful of variables/layers



```{r compare_original_clustered, message=FALSE, warning=FALSE, fig.width=8, fig.height=22, fig.show='hold'}


# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
# -   Plot comparison of original variable layers and clustered values  - #
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #
source('explore_clustering_results/compare_variables_original_DS_clustering.R')

# calculate original, DS-mean, and clustered pixel values for all variables
for (num_clusters in num_clusters_plot){
  compare_variables_original_DS_clustering(base_filepath_raster, ds_shapefile_filepath, var_weight_string, unscaled_files, variable_record_df, num_clusters, plot_flag=TRUE, pdf_flag=TRUE, subset_plot_indices=c())
} 
# plot pixel values and difference between true and clustered pixel values
plot_error_DS_clustering(base_filepath_raster, var_weight_string, num_clusters, pdf_flag=FALSE, show_error_flag=FALSE, subset_plot_indices=col_indices)


```
</br>

<b>Comparison of original pixel values (left), mean-DS values (center) and clustered values (right) for a handful of variable layers (rows). </b>

</br>






</br>

## Examine whether monthly case patterns are consistent within an archetype

### 3 clusters
```{r plot_monthly_cases_by_archetype_3clust, message=FALSE, warning=FALSE, fig.width=8, fig.height=8, fig.show='hold'}

# num_clusters = 3
# # load in clustering output for this number of clusters - kmeans_output and clara_output variables are loaded
# load(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_level_clusterResultObject_', num_clusters, 'clusters.RData', sep=''))
# 
# pdf_filename = paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/figures/mean_monthly_cases_by_archetype_', num_clusters, 'clusters.pdf', sep='')
# plot_monthly_cases_by_archetype(base_filepath_raster, ds_shapefile_filepath, num_clusters, clara_output, show_susp_flag=TRUE, show_rescaled_flag=TRUE, show_subplots_flag=TRUE, pdf_flag=FALSE, pdf_filename=pdf_filename)

```

<b>Rescaled mean case counts within each DS.</b> Each subplot shows the results for DS in an archetype.



### 5 clusters
```{r plot_monthly_cases_by_archetype_5clust, message=FALSE, warning=FALSE, fig.width=8, fig.height=12, fig.show='hold'}

# num_clusters = 5
# # load in clustering output for this number of clusters - kmeans_output and clara_output variables are loaded
# load(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_level_clusterResultObject_', num_clusters, 'clusters.RData', sep=''))
# 
# pdf_filename = paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/figures/mean_monthly_cases_by_archetype_', num_clusters, 'clusters.pdf', sep='')
# plot_monthly_cases_by_archetype(base_filepath_raster, ds_shapefile_filepath, num_clusters, clara_output, show_susp_flag=TRUE, show_rescaled_flag=TRUE, show_subplots_flag=TRUE, pdf_flag=FALSE, pdf_filename=pdf_filename)

```

<b>Rescaled mean case counts within each DS.</b> Each subplot shows the results for DS in an archetype.

  
</br>

### 60 clusters
```{r plot_monthly_cases_by_archetype_60clust, message=FALSE, warning=FALSE, fig.width=8, fig.height=16, fig.show='hold'}

num_clusters = 60
# load in clustering output for this number of clusters - kmeans_output and clara_output variables are loaded
load(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/DS_level_clusterResultObject_', num_clusters, 'clusters.RData', sep=''))


pdf_filename = paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/figures/mean_monthly_cases_by_archetype_', num_clusters, 'clusters.pdf', sep='')
plot_monthly_cases_by_archetype(base_filepath_raster, ds_shapefile_filepath, num_clusters, clara_output, show_susp_flag=TRUE, show_rescaled_flag=TRUE, show_subplots_flag=TRUE, pdf_flag=FALSE, pdf_filename=pdf_filename)

```

<b>Rescaled mean case counts within each DS.</b> Each subplot shows the results for DS in an archetype.

  
  
  
  
## How many DHS clusters are located in each archetype?
```{r num_DHS_each_archetype}

source('explore_clustering_results/num_dhs_mis_clusters_per_archetype.R')

# get list of shapefiles with DHS cluster locations for all of the survey years
#   add in the number of kids surveyed in each cluster
dhs_cluster_shapefiles_list =list()
for (yy in 1:length(dhs_cluster_locations_filepath)){
  dhs_cluster_shapefiles_list[[yy]] = shapefile(dhs_cluster_locations_filepath[yy])
  num_kids_info = read.csv(dhs_cluster_numkids_filepath[yy], as.is=TRUE)
  
  dhs_cluster_shapefiles_list[[yy]]$numU5 = NA
  
  for (kk in 1:length(num_kids_info$DHSCLUST)){
    dhs_cluster_shapefiles_list[[yy]]$numU5[which(dhs_cluster_shapefiles_list[[yy]]$DHSCLUST==num_kids_info$DHSCLUST[kk])] = num_kids_info$Number.of.Kids[kk]
  }
}


# num_clusters_plot = c(16, 22, 29, 40)
show_hist_flag = FALSE

pdf(paste(base_filepath_raster, '/clustering_results/', var_weight_string, '/figures/num_dhs_clusters_in_each_archetype.pdf', sep=''), width=10, height=8)

plot_num_clusters_per_arch(base_filepath_raster, var_weight_string, num_clusters_plot, dhs_cluster_shapefiles_list, ds_shapefile, show_hist_flag, dhs_cluster_years)

plot_num_U5_per_arch(base_filepath_raster, var_weight_string, num_clusters_plot, dhs_cluster_shapefiles_list, ds_shapefile, show_hist_flag, dhs_cluster_years)

plot_map_num_U5_per_arch(base_filepath_raster, var_weight_string, num_clusters_plot, dhs_cluster_shapefiles_list, ds_shapefile, dhs_cluster_years, plot_map_indices = c(3,5))


dev.off()

```


# Reassign DS from archetypes with insufficient U5 survey coverage
```{r reassign_undersurveyed_archetypes}

source('explore_clustering_results/reassign_undersurveyed_archetypes.R')

# specify the minimum number of individuals included in 2010 survey
min_u5_in_arch = 50

reassign_undersurveyed_archetypes(base_filepath_raster, var_weight_string, min_u5_in_arch, dhs_cluster_shapefiles_list, dhs_index_reassignment, dhs_cluster_years, ds_shapefile, num_clusters_plot, overwrite_flag=FALSE)

```



## Compare original variable values with values in reassigned archetypes

For each pixel in Burkina Faso, we can visually examine how similar the original, DS-mean, and cluster-reassigned variable values are. The below plots show a select handful of variables/layers

```{r compare_original_reassigned}


# Create plots of outputs like were done for original archetypes


var_weight_string_reassigned = paste(var_weight_string, '/reassigned', min_u5_in_arch, sep='')

# calculate original, DS-mean, and reassigned-clustered pixel values for all variables
for (num_clusters in num_clusters_plot){
  compare_variables_original_DS_clustering(base_filepath_raster, ds_shapefile_filepath, var_weight_string=var_weight_string_reassigned, unscaled_files, variable_record_df, num_clusters, plot_flag=TRUE, pdf_flag=TRUE, subset_plot_indices=c())
}

```



## How many DHS clusters are located in each archetype after reassignment?

```{r num_DHS_each_reassigned_archetype}
  
# how many U5 are surveyed in each new reassigned archetype
pdf(paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/figures/num_dhs_clusters_in_each_archetype.pdf', sep=''), width=10, height=8)

plot_num_clusters_per_arch(base_filepath_raster, var_weight_string=var_weight_string_reassigned, num_clusters_plot, dhs_cluster_shapefiles_list, ds_shapefile, show_hist_flag, dhs_cluster_years)

plot_num_U5_per_arch(base_filepath_raster, var_weight_string=var_weight_string_reassigned, num_clusters_plot, dhs_cluster_shapefiles_list, ds_shapefile, show_hist_flag, dhs_cluster_years)

plot_map_num_U5_per_arch(base_filepath_raster, var_weight_string=var_weight_string_reassigned, num_clusters_plot, dhs_cluster_shapefiles_list, ds_shapefile, dhs_cluster_years, plot_map_indices = c(3,5), print_true_num_arch_flag=TRUE)

dev.off()

```


## Determine which archetyping option performs best when compared with true variable values

```{r find_best_reassigned_option}

# calculate the mean difference between the 'true' DS value and the corresponding archetype-assigned DS value for all variables (with the weighting)

# read in the mean variable values for all of the ds (using this var_weight_string's weighting values)
weighted_var_values_each_ds = read.csv(paste(base_filepath_raster, '/aligned_layers/', var_weight_string, '/mean_variable_values_each_DS.csv', sep=''), as.is=TRUE)[,-1]  # these should be in the same order as the clara_output

mean_ss_diff = rep(NA, length(num_clusters_plot))
median_ss_diff = rep(NA, length(num_clusters_plot))
max_ss_diff = rep(NA, length(num_clusters_plot))
quantile95_ss_diff = rep(NA, length(num_clusters_plot))
quantile90_ss_diff = rep(NA, length(num_clusters_plot))
quantile80_ss_diff = rep(NA, length(num_clusters_plot))
quantile05_ss_diff = rep(NA, length(num_clusters_plot))
index = 1
sum_square_diff_all = matrix(NA, nrow=length(num_clusters_plot), ncol=length(ds_names))

for(num_clusters in num_clusters_plot){
  
  # load in information on DS assignment and archetype medoids for this number of clusters - loaded object is called 'clara_output'
  load(paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/DS_level_clusterResultObject_', num_clusters, 'clusters.RData', sep=''))
  if(!all(colnames(weighted_var_values_each_ds)[-1] == colnames(clara_output$medoids))){
    warning('variables may be misaligned between the ds values and the clustered medoid values')
  }
  # for each DS store the sum of squared differences (difference between the DS's value and the medoid's value, summed across all variable layers)
  sum_square_diff = rep(NA, length(clara_output$clustering))
  for(ds in 1:length(clara_output$clustering)){
    sum_square_diff[ds] = sum((as.vector(weighted_var_values_each_ds[ds, -1]) - clara_output$medoids[clara_output$clustering[ds],])^2)
  }
  
  mean_ss_diff[index] = mean(sum_square_diff)
  median_ss_diff[index] = median(sum_square_diff)
  max_ss_diff[index] = max(sum_square_diff)
  quantile95_ss_diff[index] = quantile(sum_square_diff, probs=c(0.95))
  quantile90_ss_diff[index] = quantile(sum_square_diff, probs=c(0.90))
  quantile80_ss_diff[index] = quantile(sum_square_diff, probs=c(0.80))
  quantile05_ss_diff[index] = quantile(sum_square_diff, probs=c(0.05))
  sum_square_diff_all[index,] = sum_square_diff
  index = index+1
}

num_clust_comparison_df = data.frame(mean_ss_diff, median_ss_diff, max_ss_diff, quantile05_ss_diff, quantile80_ss_diff, quantile90_ss_diff, quantile95_ss_diff)

```




```{r save_reassigned_representative_ds}


for(num_clusters in num_clusters_plot){
  
  # load in information on DS assignment and archetype medoids for this number of clusters - loaded object is called 'clara_output'
  load(paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/DS_level_clusterResultObject_', num_clusters, 'clusters.RData', sep=''))
  
  df_representative_ds = data.frame('DS'=ds_names, 'repDS'=ds_names[clara_output$i.med[clara_output$clustering]])
  write.csv(df_representative_ds, paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/representative_DS_orig', num_clusters, 'clusters.csv', sep=''))
  
  print(paste('orig # clusters:', num_clusters, '; new # clusters:', length(unique(clara_output$clustering))))
}



```








```{r, plot_cluster_output_reassigned, echo = FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=12}
par(mfrow=c(3,2), mar=c(0.1, 0.1, 1, 0.1))

pdf(paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/figures/map_colored_by_archetype.pdf', sep=''), width=10, height=8)


for(ii in 1:length(num_clusters_plot)){
  num_clusters=num_clusters_plot[ii]

  # load in kmeans output for this number of clusters - kmeans_output and clara_output variables are loaded
  load(paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/DS_level_clusterResultObject_', num_clusters, 'clusters.RData', sep=''))
  # get the names of the DS used as medoids
  DS_representative_ids_0 = clara_output$i.med
  # get cluster assignment for each DS
  DS_cluster_assignment_0 = clara_output$clustering
  
  # remove DS_representative_ids that aren't among the clustering output and update DS cluster assignment accordingly
  DS_representative_ids = DS_representative_ids_0[unique(DS_cluster_assignment_0)]
  DS_cluster_assignment = rep(NA, length(DS_cluster_assignment_0))
  for (cc in 1:length(DS_representative_ids)){
    # find the DS that are represented by this rep_DS and point them to the index of the rep_DS
    # original cluster number for this repDS
    orig_cluster_id = which(DS_representative_ids_0 == DS_representative_ids[cc])
    # for all DS that originally had orig_cluster_id, change the cluster id to cc
    DS_cluster_assignment[which(DS_cluster_assignment_0 == orig_cluster_id)] = cc
  }

  # plot coloring DS according to archetype DS
  clust_cols = rainbow(length(DS_representative_ids), alpha=0.2)[DS_cluster_assignment]

  
  plot(ds_shapefile, col=clust_cols, main=paste('orig # clusters:', num_clusters, '; new # clusters:', length(unique(clara_output$clustering))))

}
dev.off()




pdf(paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/figures/map_colored_by_archetype_dark_repDS.pdf', sep=''), width=10, height=8)


for(ii in 1:length(num_clusters_plot)){
  num_clusters=num_clusters_plot[ii]

  # load in kmeans output for this number of clusters - kmeans_output and clara_output variables are loaded
  load(paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/DS_level_clusterResultObject_', num_clusters, 'clusters.RData', sep=''))
  # get the names of the DS used as medoids
  DS_representative_ids_0 = clara_output$i.med
  # get cluster assignment for each DS
  DS_cluster_assignment_0 = clara_output$clustering
  
  # remove DS_representative_ids that aren't among the clustering output and update DS cluster assignment accordingly
  DS_representative_ids = DS_representative_ids_0[unique(DS_cluster_assignment_0)]
  DS_cluster_assignment = rep(NA, length(DS_cluster_assignment_0))
  for (cc in 1:length(DS_representative_ids)){
    # find the DS that are represented by this rep_DS and point them to the index of the rep_DS
    # original cluster number for this repDS
    orig_cluster_id = which(DS_representative_ids_0 == DS_representative_ids[cc])
    # for all DS that originally had orig_cluster_id, change the cluster id to cc
    DS_cluster_assignment[which(DS_cluster_assignment_0 == orig_cluster_id)] = cc
  }

  # plot coloring DS according to archetype DS
  clust_cols = rainbow(length(DS_representative_ids), alpha=0.2)[DS_cluster_assignment]
  clust_cols[DS_representative_ids] = rainbow(length(DS_representative_ids), alpha=0.6)

  
  plot(ds_shapefile, col=clust_cols, main=paste('orig # clusters:', num_clusters, '; new # clusters:', length(unique(clara_output$clustering))))

}
dev.off()




pdf(paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/figures/map_colored_by_archetype_labelRepDS.pdf', sep=''), width=10, height=8)


for(ii in 1:length(num_clusters_plot)){
  num_clusters=num_clusters_plot[ii]

  # load in kmeans output for this number of clusters - kmeans_output and clara_output variables are loaded
  load(paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/DS_level_clusterResultObject_', num_clusters, 'clusters.RData', sep=''))
  # get the names of the DS used as medoids
  DS_representative_ids_0 = clara_output$i.med
  # get cluster assignment for each DS
  DS_cluster_assignment_0 = clara_output$clustering
  
  # remove DS_representative_ids that aren't among the clustering output and update DS cluster assignment accordingly
  DS_representative_ids = DS_representative_ids_0[unique(DS_cluster_assignment_0)]
  DS_cluster_assignment = rep(NA, length(DS_cluster_assignment_0))
  for (cc in 1:length(DS_representative_ids)){
    # find the DS that are represented by this rep_DS and point them to the index of the rep_DS
    # original cluster number for this repDS
    orig_cluster_id = which(DS_representative_ids_0 == DS_representative_ids[cc])
    # for all DS that originally had orig_cluster_id, change the cluster id to cc
    DS_cluster_assignment[which(DS_cluster_assignment_0 == orig_cluster_id)] = cc
  }

  # plot coloring DS according to archetype DS
  clust_cols = rainbow(length(DS_representative_ids), alpha=0.2)[DS_cluster_assignment]
  clust_cols[DS_representative_ids] = rainbow(length(DS_representative_ids), alpha=0.5)

  
  plot(ds_shapefile, col=clust_cols, main=paste('orig # clusters:', num_clusters, '; new # clusters:', length(unique(clara_output$clustering))), border=rgb(0.7,0.7,0.7))
  
  rep_ds_coordinates = coordinates(ds_shapefile)[DS_representative_ids,]
  # jitter Isoko South a bit upwards to avoid label overlap
  north_shift = rep(0,length(DS_representative_ids))
  north_shift[which(ds_names[DS_representative_ids] == 'Isoko South')] = 0.13
  text(ds_names[DS_representative_ids], x=rep_ds_coordinates[,1], y=rep_ds_coordinates[,2]+north_shift)


}
dev.off()



pdf(paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/figures/map_colored_by_archetype_labelRepDS_v2.pdf', sep=''), width=10, height=8)


for(ii in 1:length(num_clusters_plot)){
  num_clusters=num_clusters_plot[ii]

  # load in kmeans output for this number of clusters - kmeans_output and clara_output variables are loaded
  load(paste(base_filepath_raster, '/clustering_results/', var_weight_string_reassigned, '/DS_level_clusterResultObject_', num_clusters, 'clusters.RData', sep=''))
  # get the names of the DS used as medoids
  DS_representative_ids_0 = clara_output$i.med
  # get cluster assignment for each DS
  DS_cluster_assignment_0 = clara_output$clustering
  
  # remove DS_representative_ids that aren't among the clustering output and update DS cluster assignment accordingly
  DS_representative_ids = DS_representative_ids_0[unique(DS_cluster_assignment_0)]
  DS_cluster_assignment = rep(NA, length(DS_cluster_assignment_0))
  for (cc in 1:length(DS_representative_ids)){
    # find the DS that are represented by this rep_DS and point them to the index of the rep_DS
    # original cluster number for this repDS
    orig_cluster_id = which(DS_representative_ids_0 == DS_representative_ids[cc])
    # for all DS that originally had orig_cluster_id, change the cluster id to cc
    DS_cluster_assignment[which(DS_cluster_assignment_0 == orig_cluster_id)] = cc
  }

  # plot coloring DS according to archetype DS
  clust_cols = rainbow(length(DS_representative_ids), alpha=0.2)[DS_cluster_assignment]
  rep_ds_points_cols = rainbow(length(DS_representative_ids), alpha=1)

  
  plot(ds_shapefile, col=clust_cols, main=paste('orig # clusters:', num_clusters, '; new # clusters:', length(unique(clara_output$clustering))), border=rgb(0.7,0.7,0.7))
  
  rep_ds_coordinates = coordinates(ds_shapefile)[DS_representative_ids,]
  points(x=rep_ds_coordinates[,1], y=rep_ds_coordinates[,2], cex=2, pch=20, col=rep_ds_points_cols)
  
  # jitter Isoko South a bit upwards to avoid label overlap
  north_shift = rep(0,length(DS_representative_ids))
  north_shift[which(ds_names[DS_representative_ids] == 'Isoko South')] = 0.13
  text(ds_names[DS_representative_ids], x=rep_ds_coordinates[,1]+0.01, y=rep_ds_coordinates[,2]+north_shift, pos=4)

}
dev.off()

par(mfrow=c(1,1))

```



